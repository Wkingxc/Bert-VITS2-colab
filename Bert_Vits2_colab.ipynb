{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT1ZvJc4-Nks",
        "outputId": "17e09c1e-e7e7-4972-854e-ca9529625f6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Bert-Vits2-Colab'...\n",
            "remote: Enumerating objects: 408, done.\u001b[K\n",
            "remote: Counting objects: 100% (408/408), done.\u001b[K\n",
            "remote: Compressing objects: 100% (257/257), done.\u001b[K\n",
            "remote: Total 408 (delta 140), reused 392 (delta 125), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (408/408), 11.54 MiB | 15.15 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Wkingxc/Bert-Vits2-Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_iCymiAOzhk",
        "outputId": "2bc33ab8-ddc8-40d6-aee1-576a47a7321f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9SgDIpKH91ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb102d44-6bc3-4b21-9a65-3f4e5d229ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Bert-Vits2-Colab\n"
          ]
        }
      ],
      "source": [
        "c_name = \"Xier\"\n",
        "c_voice = \"Xier.zip\"\n",
        "# 创建目录\n",
        "!mkdir -p /content/Bert-Vits2-Colab/Data/$c_name/filelists/\n",
        "!mkdir -p /content/Bert-Vits2-Colab/Data/$c_name/models/\n",
        "!mkdir -p /content/Bert-Vits2-Colab/Data/$c_name/raw/$c_name/\n",
        "!mkdir -p /content/Bert-Vits2-Colab/Data/$c_name/wavs/\n",
        "# 从谷歌硬盘拷贝音频和台词文件\n",
        "%cd /content/Bert-Vits2-Colab\n",
        "!cp /content/drive/MyDrive/$c_voice /content/Bert-Vits2-Colab/Data/$c_name/raw/$c_name/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKDi9zGmRZps"
      },
      "outputs": [],
      "source": [
        "!unzip /content/Bert-Vits2-Colab/Data/$c_name/raw/$c_name/$c_voice -d /content/Bert-Vits2-Colab/Data/$c_name/raw/$c_name/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "atRCCFpoTd6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9affa76d-0e53-4ef0-a5c4-35629c60c0db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for amfm_decompy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r ./requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKmJqXLVAsny"
      },
      "outputs": [],
      "source": [
        "!echo \"=====下载 bert=====\"\n",
        "!wget -P ./bert/chinese-roberta-wwm-ext-large/ https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/pytorch_model.bin\n",
        "!wget -P ./bert/deberta-v2-large-japanese-char-wwm/ https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm/resolve/main/pytorch_model.bin\n",
        "!wget -P ./bert/deberta-v3-large/ https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.bin\n",
        "!wget -P ./bert/deberta-v3-large/ https://huggingface.co/microsoft/deberta-v3-large/resolve/main/pytorch_model.generator.bin\n",
        "!wget -P ./bert/deberta-v3-large/ https://huggingface.co/microsoft/deberta-v3-large/resolve/main/spm.model\n",
        "!wget -P ./bert/Erlangshen-MegatronBert-1.3B-Chinese https://huggingface.co/IDEA-CCNL/Erlangshen-MegatronBert-1.3B/resolve/main/pytorch_model.bin\n",
        "# !cp /content/drive/MyDrive/pytorch_model.bin /content/Bert-Vits2-Colab/bert/Erlangshen-MegatronBert-1.3B-Chinese/\n",
        "\n",
        "!echo \"====下载slm====\"\n",
        "!wget -P ./slm/wavlm-base-plus/ https://huggingface.co/microsoft/wavlm-base-plus/resolve/main/pytorch_model.bin\n",
        "!wget -P ./emotional/clap-htsat-fused https://huggingface.co/laion/clap-htsat-fused/resolve/main/pytorch_model.bin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5SVZKwfJWJC",
        "outputId": "b33146c1-a1db-44ee-81f9-c6769b7b68be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan  7 12:07:11 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "/content/Bert-Vits2-Colab\n"
          ]
        }
      ],
      "source": [
        "# 下面正式开始训练步骤\n",
        "!nvidia-smi\n",
        "!pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OljFOvDPUZjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083c780b-079f-45e2-ac3f-bc02fafeb0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parent directory: Data/Xier/raw\n",
            "Data/Xier/raw/Xier\n",
            "519it [00:08, 60.17it/s]\n",
            "finished.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python trans_wav_lab.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BGxyMLLOI4Uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1257c29-0411-4dd8-e826-7964bec1f5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "  0% 0/519 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.677 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 519/519 [00:04<00:00, 106.20it/s]\n",
            "总重复音频数：0，总未找到的音频数:0\n",
            "训练集和验证集生成完成！\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python preprocess_text.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DOAg3awaLSKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5694f7-6359-43f1-ef7b-00cfe3978134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 519/519 [00:52<00:00,  9.87it/s]\n",
            "bert生成完毕!, 共有519个bert.pt生成!\n"
          ]
        }
      ],
      "source": [
        "!python bert_gen.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vTLoqtUSMgTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9742e0bc-6293-4df2-9596-1fe72641c727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/519 [00:00<?, ?it/s]/content/Bert-Vits2-Colab/clap_gen.py:34: FutureWarning: Pass sr=48000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  audio = librosa.load(wav_path, 48000)[0]\n",
            "/content/Bert-Vits2-Colab/clap_gen.py:34: FutureWarning: Pass sr=48000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  audio = librosa.load(wav_path, 48000)[0]\n",
            "100% 519/519 [07:04<00:00,  1.22it/s]\n",
            "clap生成完毕!, 共有519个emo.pt生成!\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python clap_gen.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P \"./Data/$c_name/models/\" https://huggingface.co/guiyun/Bert-VITS2-chinese/resolve/main/D_0.pth\n",
        "!wget -P \"./Data/$c_name/models/\" https://huggingface.co/guiyun/Bert-VITS2-chinese/resolve/main/G_0.pth\n",
        "!wget -P \"./Data/$c_name/models/\" https://huggingface.co/guiyun/Bert-VITS2-chinese/resolve/main/WD_0.pth"
      ],
      "metadata": {
        "id": "PObA0WqsVT-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==2.8.0"
      ],
      "metadata": {
        "id": "_WIwQYpIqcA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_ms.py"
      ],
      "metadata": {
        "id": "kneNJH-iUhOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe5583bc-506b-4661-cb61-effc18eaea19"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-07 12:20:45.557865: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-07 12:20:45.557971: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-07 12:20:45.667282: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-07 12:20:45.897036: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-07 12:20:47.883132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "加载config中的配置0\n",
            "加载config中的配置localhost\n",
            "加载config中的配置10086\n",
            "加载config中的配置0\n",
            "加载config中的配置1\n",
            "加载环境变量 \n",
            "MASTER_ADDR: localhost,\n",
            "MASTER_PORT: 10086,\n",
            "WORLD_SIZE: 1,\n",
            "RANK: 0,\n",
            "LOCAL_RANK: 0\n",
            "\u001b[32m01-07 12:20:54\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:65 | Init dataset...\n",
            "100% 515/515 [00:00<00:00, 45395.76it/s]\n",
            "\u001b[32m01-07 12:20:54\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:80 | skipped: 0, total: 515\n",
            "\u001b[32m01-07 12:20:54\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:65 | Init dataset...\n",
            "100% 4/4 [00:00<00:00, 24105.20it/s]\n",
            "\u001b[32m01-07 12:20:54\u001b[0m \u001b[1mINFO     \u001b[0m| data_utils.py:80 | skipped: 0, total: 4\n",
            "Using noise scaled MAS for VITS2\n",
            "INFO:models:Loaded checkpoint 'Data/Xier/models/WD_0.pth' (iteration 0)\n",
            "ERROR:models:emb_g.weight is not in the checkpoint\n",
            "INFO:models:Loaded checkpoint 'Data/Xier/models/G_0.pth' (iteration 0)\n",
            "PytorchStreamReader failed reading zip archive: failed finding central directory\n",
            "Some weights of the model checkpoint at ./slm/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of WavLMModel were not initialized from the model checkpoint at ./slm/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0% 0/129 [00:00<?, ?it/s]INFO:models:Train Epoch: 1 [0%]\n",
            "INFO:models:[6.016129970550537, 5.2900285720825195, 0.4696632921695709, 24.206302642822266, 2.943246364593506, 3.936175584793091, 0, 0.0001]\n",
            "Evaluating ...\n",
            "INFO:models:Saving model and optimizer state at iteration 1 to Data/Xier/models/G_0.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 1 to Data/Xier/models/D_0.pth\n",
            "INFO:models:Saving model and optimizer state at iteration 1 to Data/Xier/models/WD_0.pth\n",
            "  4% 5/129 [00:30<12:45,  6.17s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Bert-Vits2-Colab/train_ms.py\", line 833, in <module>\n",
            "    run()\n",
            "  File \"/content/Bert-Vits2-Colab/train_ms.py\", line 371, in run\n",
            "    train_and_evaluate(\n",
            "  File \"/content/Bert-Vits2-Colab/train_ms.py\", line 593, in train_and_evaluate\n",
            "    scaler.scale(loss_gen_all).backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}